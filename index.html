
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  
    <title>我是小韩同学</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="洛绍川">
    

    
    <meta name="description" content="code">
<meta property="og:type" content="website">
<meta property="og:title" content="我是小韩同学">
<meta property="og:url" content="http://www.luoshaochuan.com/index.html">
<meta property="og:site_name" content="我是小韩同学">
<meta property="og:description" content="code">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="我是小韩同学">
<meta name="twitter:description" content="code">

    
    
    
    <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="我是小韩同学" title="我是小韩同学"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="我是小韩同学">我是小韩同学</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/tags">标签</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:www.luoshaochuan.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/27/强化学习入门(三)/" title="强化学习入门(三)" itemprop="url">强化学习入门(三)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-27T13:48:13.000Z" itemprop="datePublished"> 发表于 2017-12-27</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>书籍：机器学习 周志华<br>视频：<a href="https://www.youtube.com/watch?v=sGuiWX07sKw" target="_blank" rel="external">RL Course by David Silver</a><br>博客：<a href="http://www.algorithmdog.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B" target="_blank" rel="external">AlgorithmDog</a><br>知乎：<a href="https://zhuanlan.zhihu.com/p/26007538" target="_blank" rel="external">强化学习入门 第五讲 值函数逼近</a></p>
<h3 id="值函数近似"><a href="#值函数近似" class="headerlink" title="值函数近似"></a>值函数近似</h3><blockquote>
<p>前面已经讲了强化学习的基本方法：基于动态规划的方法，基于蒙特卡罗的方法和基于时间差分的方法。这些方法有一个基本的前提条件，那就是状态空间和动作空间是离散的，而且状态空间和动作空间不能太大。</p>
</blockquote>
<p><strong>强化学习的基本步骤：首先评估值函数，接着利用值函数改进当前策略，值函数的评估是关键。</strong>  </p>
<p>在表格型强化学习中，值函数对应着一张表。在值函数逼近方法中，值函数对应着一个逼近函数。值函数近似分为参数化近似和非参数化近似。  </p>
<p><strong>随机梯度近似</strong>：<br><img src="http://oc9b84kcd.bkt.clouddn.com/SGD.png" alt=""></p>
<h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>Q-Learning是异策略时间差分方法，伪代码如图所示：<br><img src="http://oc9b84kcd.bkt.clouddn.com/Q.jpg" alt=""> </p>
<h4 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h4><p><img src="http://oc9b84kcd.bkt.clouddn.com/DQN2.png" alt=""><br>DQN对Q-learning的修改主要体现在以下三个方面：</p>
<ul>
<li><p>DQN利用深度卷积神经网络逼近值函数</p>
</li>
<li><p>DQN利用了经验回放对强化学习的学习过程进行训练</p>
</li>
<li><p>DQN独立设置了目标网络来单独处理时间差分算法中的TD偏差。</p>
<h5 id="1-DQN利用卷积神经网络逼近行为值函数"><a href="#1-DQN利用卷积神经网络逼近行为值函数" class="headerlink" title="1. DQN利用卷积神经网络逼近行为值函数"></a>1. DQN利用卷积神经网络逼近行为值函数</h5><p><img src="http://oc9b84kcd.bkt.clouddn.com/CNN.png" alt=""><br><strong>step1：首先，我们使用参数化逼近来近似行为值函数</strong>：</p>
<span>$Q(s,a;w)\approx Q^\prime(s,a)$</span><!-- Has MathJax -->
<p><strong>step2: 在Q值中使用均方误差来定义目标函数的loss function</strong>:</p>
<span>$L(w) = E[(\color{red}{\underbrace{r+\gamma\max_{a^\prime}Q(s^\prime,a^\prime,w)}_{Target}}\color{black}-Q(s,a,w))^2]$</span><!-- Has MathJax -->
<p><strong>step3: 计算参数ω关于loss function梯度</strong>：</p>
<span>$\frac{\partial  L(w)}{\partial w} =  E[(\color{red}{r+\gamma\max_{a^\prime}Q(s^\prime,a^\prime,w)}\color{black}-Q(s,a,w))\frac{\partial Q(s,a,w)}{\partial w}]$</span><!-- Has MathJax -->
<p><strong>step4: 使用SGD实现End-to-end的优化目标</strong>：<br>有了上面的梯度，可以从深度神经网络中进行计算，因此，就可以使用SGD 随机梯度下降来更新参数，从而得到最优的Q值。</p>
</li>
</ul>
<h5 id="2-DQN利用Expericence-Replay对强化学习过程进行训练"><a href="#2-DQN利用Expericence-Replay对强化学习过程进行训练" class="headerlink" title="2.DQN利用Expericence Replay对强化学习过程进行训练"></a>2.DQN利用Expericence Replay对强化学习过程进行训练</h5><p><strong>Experience Replay 的动机是: 解决神经网络不收敛</strong><br>1）深度神经网络作为有监督学习模型，要求数据满足独立同分布<br>2）但 Q Learning 算法得到的样本前后是有关系的。为了打破数据之间的关联性，Experience Replay 方法通过存储-采样的方法将这个关联性打破了。</p>
<h5 id="3-DQN设置了目标网络来单独处理时间差分算法中的TD偏差"><a href="#3-DQN设置了目标网络来单独处理时间差分算法中的TD偏差" class="headerlink" title="3.DQN设置了目标网络来单独处理时间差分算法中的TD偏差"></a>3.DQN设置了目标网络来单独处理时间差分算法中的TD偏差</h5><p><strong>进一步打破相关性</strong><br>以往用梯度下降算法：<br><span>$\omega_{t+1} =  \omega_{t}+\alpha[(\color{red}{r+\gamma\max_{a^\prime}Q(s^\prime,a^\prime,w)}\color{black}-Q(s,a,w))\frac{\partial Q(s,a,w)}{\partial w}$</span><!-- Has MathJax --><br>计算TD目标(红色部分)的动作值函数所用的网络参数ω，与梯度计算中要逼近的值函数所用的网络参数相同，这样就容易使得数据间存在关联性，训练不稳定。<br>为了解决这个问题，DeepMind在TD Target采用和训练目标不一样的参数ω-：<br><span>$\omega_{t+1} =  \omega_{t}+\alpha[(\color{red}{r+\gamma\max_{a^\prime}Q(s^\prime,a^\prime,w^-)}\color{black}-Q(s,a,w))\frac{\partial Q(s,a,w)}{\partial w}$</span><!-- Has MathJax --><br><a href="http://blog.csdn.net/songrotek/article/details/50951537" target="_blank" rel="external"><strong>参考代码</strong>：</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</div><div class="line"></div><div class="line"><span class="comment">#超参数</span></div><div class="line">FRAME_PER_ACTION=<span class="number">1</span>    <span class="comment">#每个动作采几帧</span></div><div class="line">GAMMA=<span class="number">0.99</span>            <span class="comment">#折扣率</span></div><div class="line">OBSERVE=<span class="number">100.</span>          <span class="comment">#训练之前的观察时间</span></div><div class="line">EXPLORE=<span class="number">10000.</span>       <span class="comment">#试验数</span></div><div class="line">FINAL_EPSILON=<span class="number">0.0</span>     <span class="comment">#epsilon贪心最终参数</span></div><div class="line">INITIAL_EPSILON = <span class="number">0.5</span> <span class="comment">#epsilon贪心初始参数</span></div><div class="line">REPLAY_MEMORY=<span class="number">50000</span>   <span class="comment">#回放队列大小</span></div><div class="line">BATCH_SIZE=<span class="number">32</span>         <span class="comment">#minbatch大小</span></div><div class="line">UPDATE_TIME=<span class="number">100</span>       <span class="comment">#更新目标Q网络的时间间隔</span></div><div class="line">LEARNING_RATE=<span class="number">0.001</span>   <span class="comment">#学习率</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrainDQN</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,actions)</span>:</span></div><div class="line">        <span class="comment">#初始化回放池</span></div><div class="line">        self.replayMemory = deque()</div><div class="line">        <span class="comment">#初始化相关参数</span></div><div class="line">        self.timeStep = <span class="number">0</span></div><div class="line">        self.epsilon=INITIAL_EPSILON</div><div class="line">        self.actions=actions</div><div class="line">        <span class="comment">#初始化Q网络</span></div><div class="line">        self.stateInput,self.QValue,self.W_conv1,self.b_conv1,self.W_conv2,self.b_conv2,self.W_conv3,self.b_conv3,self.W_fc1,self.b_fc1,self.W_fc2,self.b_fc2 = self.createQNetwork()</div><div class="line">        <span class="comment">#初始化目标Q网络并设置目标Q网络和Q网络参数一样，TD Target主要用于缓存Q网络参数值</span></div><div class="line">        self.stateInputT,self.QValueT,self.W_conv1T,self.b_conv1T,self.W_conv2T,self.b_conv2T,self.W_conv3T,self.b_conv3T,self.W_fc1T,self.b_fc1T,self.W_fc2T,self.b_fc2T = self.createQNetwork()</div><div class="line">        self.copyTargetQNetworkOperation = [self.W_conv1T.assign(self.W_conv1),self.b_conv1T.assign(self.b_conv1),self.W_conv2T.assign(self.W_conv2),self.b_conv2T.assign(self.b_conv2),self.W_conv3T.assign(self.W_conv3),self.b_conv3T.assign(self.b_conv3),self.W_fc1T.assign(self.W_fc1),self.b_fc1T.assign(self.b_fc1),self.W_fc2T.assign(self.W_fc2),self.b_fc2T.assign(self.b_fc2)]</div><div class="line">        self.createTrainingMethod()</div><div class="line">        <span class="comment"># saving and loading networks</span></div><div class="line">        self.saver = tf.train.Saver()</div><div class="line">        self.session = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</div><div class="line">        self.session.run(tf.initialize_all_variables())</div><div class="line">        checkpoint = tf.train.get_checkpoint_state(<span class="string">"saved_networks"</span>)</div><div class="line">        <span class="keyword">if</span> checkpoint <span class="keyword">and</span> checkpoint.model_checkpoint_path:</div><div class="line">            self.saver.restore(self.session, checkpoint.model_checkpoint_path)</div><div class="line">            <span class="keyword">print</span> (<span class="string">"Successfully loaded:"</span>, checkpoint.model_checkpoint_path)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">print</span> (<span class="string">"Could not find old network weights"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createQNetwork</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment">#三个卷积层</span></div><div class="line">        W_conv1=self.weight_variable([<span class="number">8</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">32</span>])</div><div class="line">        b_conv1=self.bias_variable([<span class="number">32</span>])</div><div class="line"></div><div class="line">        W_conv2=self.weight_variable([<span class="number">4</span>,<span class="number">4</span>,<span class="number">32</span>,<span class="number">64</span>])</div><div class="line">        b_conv2=self.bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line">        W_conv3=self.weight_variable([<span class="number">3</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>])</div><div class="line">        b_conv3=self.bias_variable([<span class="number">64</span>])</div><div class="line">        <span class="comment">#两个全连接层</span></div><div class="line">        W_fc1=self.weight_variable([<span class="number">1600</span>,<span class="number">512</span>])</div><div class="line">        b_fc1=self.bias_variable([<span class="number">512</span>])</div><div class="line"></div><div class="line">        W_fc2=self.weight_variable([<span class="number">512</span>,self.actions])</div><div class="line">        b_fc2=self.bias_variable([self.actions])</div><div class="line">        <span class="comment">#输入层</span></div><div class="line">        stateInput=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">80</span>,<span class="number">80</span>,<span class="number">4</span>])<span class="comment">#状态为4张80*80的照片</span></div><div class="line">        <span class="comment">#隐藏层</span></div><div class="line">        h_conv1=tf.nn.relu(self.conv2d(stateInput,W_conv1,<span class="number">4</span>)+b_conv1)</div><div class="line">        h_pool1=self.max_pool_2x2(h_conv1)</div><div class="line">        h_conv2 = tf.nn.relu(self.conv2d(h_pool1,W_conv2,<span class="number">2</span>) + b_conv2)</div><div class="line"></div><div class="line">        h_conv3 = tf.nn.relu(self.conv2d(h_conv2,W_conv3,<span class="number">1</span>) + b_conv3)</div><div class="line"></div><div class="line">        h_conv3_flat = tf.reshape(h_conv3,[<span class="number">-1</span>,<span class="number">1600</span>])</div><div class="line">        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat,W_fc1) + b_fc1)</div><div class="line">        <span class="comment"># Q value 层</span></div><div class="line">        QValue = tf.matmul(h_fc1,W_fc2)+b_fc2</div><div class="line"></div><div class="line">        <span class="keyword">return</span> stateInput,QValue,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2</div><div class="line"></div><div class="line">    <span class="comment">#缓存Q网络</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">copyTargetQNetwork</span><span class="params">(self)</span>:</span></div><div class="line">        self.session.run(self.copyTargetQNetworkOperation)</div><div class="line"></div><div class="line">    <span class="comment">#把观察的图片作为状态</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setInitState</span><span class="params">(self,observation)</span>:</span></div><div class="line">        self.currentState = np.stack((observation, observation, observation, observation), axis = <span class="number">2</span>)</div><div class="line">    <span class="comment">#记住输出是Q值，关键要计算出cost，里面关键是计算Q_action的值，即该state和action下的Q值。由于actionInput是</span></div><div class="line">    <span class="comment">#one hot vector的形式，因此tf.mul(self.QValue, self.actionInput)正好就是该action下的Q值。</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createTrainingMethod</span><span class="params">(self)</span>:</span></div><div class="line">        self.actionInput=tf.placeholder(tf.float32,[<span class="keyword">None</span>,self.actions])</div><div class="line">        self.yInput=tf.placeholder(tf.float32,[<span class="keyword">None</span>])</div><div class="line">        Q_Action=tf.reduce_sum(tf.multiply(self.QValue,self.actionInput),reduction_indices=<span class="number">1</span>)</div><div class="line">        self.cost=tf.reduce_mean(tf.square(self.yInput-Q_Action))</div><div class="line">        self.trainStep=tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.cost)</div><div class="line"></div><div class="line">    <span class="comment">#主要是从回放池里随机挑选一批数据，用TD Q网络求值，然后用于更新Q网络参数</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trainQNetwork</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment">#step 1:从经验池里获取随机采样</span></div><div class="line">        minibatch=random.sample(self.replayMemory,BATCH_SIZE)</div><div class="line">        state_batch=[data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">        action_batch=[data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">        reward_batch=[data[<span class="number">2</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">        nextState_batch=[data[<span class="number">3</span>]<span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">        <span class="comment">#step 2:计算y</span></div><div class="line">        y_batch=[]</div><div class="line">        QValue_batch=self.QValueT.eval(feed_dict=&#123;self.stateInputT:nextState_batch&#125;)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,BATCH_SIZE):</div><div class="line">            terminal=minibatch[i][<span class="number">4</span>]</div><div class="line">            <span class="keyword">if</span> terminal:</div><div class="line">                y_batch.append(reward_batch[i])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                y_batch.append(reward_batch[i]+GAMMA*np.max(QValue_batch[i]))</div><div class="line">        self.trainStep.run(feed_dict=&#123;</div><div class="line">            self.yInput:y_batch,</div><div class="line">            self.actionInput:action_batch,</div><div class="line">            self.stateInput:state_batch</div><div class="line">            &#125;)</div><div class="line">        <span class="comment">#保存网络</span></div><div class="line">        <span class="keyword">if</span>(self.timeStep%<span class="number">10000</span>==<span class="number">0</span>):</div><div class="line">            self.saver.save(self.session,<span class="string">'saved_networks/'</span>+<span class="string">'network'</span>+<span class="string">'-dqn'</span>,global_step=self.timeStep)</div><div class="line">        <span class="comment">#更新TD Q网络</span></div><div class="line">        <span class="keyword">if</span> self.timeStep%UPDATE_TIME==<span class="number">0</span>:</div><div class="line">            self.copyTargetQNetwork()</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setPerception</span><span class="params">(self,nextObservation,action,reward,terminal)</span>:</span></div><div class="line">        newState=np.append(self.currentState[:,:,<span class="number">1</span>:],nextObservation,axis=<span class="number">2</span>)</div><div class="line">        self.replayMemory.append((self.currentState,action,reward,newState,terminal))</div><div class="line">        <span class="keyword">if</span> len(self.replayMemory)&gt;REPLAY_MEMORY:</div><div class="line">            self.replayMemory.popleft()</div><div class="line">        <span class="keyword">if</span> self.timeStep&gt;OBSERVE:</div><div class="line">            self.trainQNetwork()</div><div class="line">        <span class="comment">#打印信息</span></div><div class="line">        state=<span class="string">""</span></div><div class="line">        <span class="keyword">if</span> self.timeStep&lt;=OBSERVE:</div><div class="line">            state=<span class="string">"observe"</span></div><div class="line">        <span class="keyword">elif</span> self.timeStep&gt;OBSERVE <span class="keyword">and</span> self.timeStep&lt;=OBSERVE+EXPLORE:</div><div class="line">            state=<span class="string">"explore"</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state=<span class="string">"train"</span></div><div class="line">        <span class="keyword">print</span> (<span class="string">"TIMESTEP"</span>, self.timeStep, <span class="string">"/ STATE"</span>, state, \</div><div class="line">            <span class="string">"/ EPSILON"</span>, self.epsilon)</div><div class="line">        self.currentState = newState</div><div class="line">        self.timeStep += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment">#采用epsilon贪心获取动作</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">        QValue=self.QValue.eval(feed_dict=&#123;self.stateInput:[self.currentState]&#125;)[<span class="number">0</span>]<span class="comment">#所有动作的动作值函数</span></div><div class="line">        action=np.zeros(self.actions)</div><div class="line">        action_index=<span class="number">0</span></div><div class="line">        <span class="keyword">if</span> self.timeStep%FRAME_PER_ACTION==<span class="number">0</span>:</div><div class="line">            <span class="keyword">if</span>(random.random()&lt;=self.epsilon):</div><div class="line">                action_index=random.randrange(self.actions)</div><div class="line">                action[action_index]=<span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            action[<span class="number">0</span>]=<span class="number">1</span><span class="comment">#do noting</span></div><div class="line">        <span class="comment">#change episilon</span></div><div class="line">        <span class="keyword">if</span> self.epsilon&gt;FINAL_EPSILON <span class="keyword">and</span> self.timeStep&gt;OBSERVE:</div><div class="line">            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/EXPLORE</div><div class="line">        <span class="keyword">return</span> action</div><div class="line">    <span class="comment">#初始化状态</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setInitState</span><span class="params">(self,observation)</span>:</span></div><div class="line">        self.currentState=np.stack((observation,observation,observation,observation),axis=<span class="number">2</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self,shape)</span>:</span></div><div class="line">        initial = tf.truncated_normal(shape, stddev = <span class="number">0.01</span>)</div><div class="line">        <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self,shape)</span>:</span></div><div class="line">        initial = tf.constant(<span class="number">0.01</span>, shape = shape)</div><div class="line">        <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self,x, W, stride)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, stride, stride, <span class="number">1</span>], padding = <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self,x)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">"SAME"</span>)</div></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/强化学习/">强化学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/23/强化学习入门(二)/" title="强化学习入门(二)" itemprop="url">强化学习入门(二)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-23T13:48:13.000Z" itemprop="datePublished"> 发表于 2017-12-23</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>书籍：机器学习 周志华<br>视频：<a href="https://www.youtube.com/watch?v=sGuiWX07sKw" target="_blank" rel="external">RL Course by David Silver</a><br>博客：<a href="http://www.algorithmdog.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B" target="_blank" rel="external">AlgorithmDog</a><br><a href="http://chenrudan.github.io/blog/2016/07/11/reinforcementlearninglesssion4.html" target="_blank" rel="external">David Silver强化学习公开课之四</a></p>
<h3 id="model-free-learning-免模型学习"><a href="#model-free-learning-免模型学习" class="headerlink" title="model-free learning 免模型学习"></a>model-free learning 免模型学习</h3><p>免模型学习：状态转移、奖励函数未知<br>策略评估：蒙特卡罗、时间差分<br>策略学习：Q-Learning、Sarsa<br>强化学习的方法就是<strong>先对当前策略进行策略评估，也就是说计算出当前策略所对应的值函数；然后，利用值函数改进当前策略。</strong><br>==<strong>强化学习的关键在于计算出值函数，而值函数的关键在于计算当前状态下获得回报的期望。</strong>==  </p>
<p>在有模型的强化学习中，我们知道马尔科夫决策过程的状态转移和奖励函数，因此值函数可以用动态规划求得。<br><span>$$v_\pi(s) = \mathbb E_\pi[G_t | S_t = s]

= \sum_{a} \pi(a|s) \left[ {R}_s^a + \gamma \sum_{s&apos;} { P}_{ss&apos;}^a v_\pi(s&apos;) \right]$$</span><!-- Has MathJax --><br><span>$$q_\pi(s, a) = \mathbb E_\pi[G_t | S_t = s, A_t = a]

= {R}_s^a + \gamma \sum_{s&apos;}{P}_{ss&apos;}^a \sum_{a&apos;}\pi(a&apos;|s&apos;)q_\pi(s&apos;, a&apos;)$$</span><!-- Has MathJax --><br><strong>所以无模型学习的关键是估计状态值函数和行为值函数中的期望。</strong></p>
<h4 id="蒙特卡罗学习"><a href="#蒙特卡罗学习" class="headerlink" title="蒙特卡罗学习"></a>蒙特卡罗学习</h4><p><strong>蒙特卡罗方法是利用经验平均代替随机变量的期望,即假设每个状态的值函数取值等于多次试验返回的回报的平均值。</strong><br>评估当前策略时，我们可以利用策略产生很多次试验，每次试验都是从任意的初始状态开始直到终止状态，比如一次试验(an<br>episode)为：<br><span>$S_1,A_1,R_2,&hellip;,S_k$</span><!-- Has MathJax --><br>计算一次试验中状态s处的折扣回报返回值为：<br><span>$G_t = R_{t+1}+\gamma R_{t+2}+&hellip;+\gamma^{k-1}R_{t+k}$</span><!-- Has MathJax --><br>我们只需要计算试验的次数和每次试验返回的回报求平均即可。令N(St)表示St访问的次数，Gt表示这次实验获得的回报,则值函数的更新如下：<br><span>$N(S_t)=N(S_t)+1$</span><!-- Has MathJax --><br><span>$V(S_t) = V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))$</span><!-- Has MathJax --><br>在一些问题中，经常用一个平均访问次数alpha来代替N(St),即<br><span>$V(S_t) = V(S_t)+\alpha(G_t-V(S_t))$</span><!-- Has MathJax --></p>
<p><strong>探索</strong>：<br>为了获得较好的指函数的估计，需要充分探索每一个每一个动作可能到达的状态，也就是说在任意状态下，采用动作集中每个动作的概率都大于零。可以采用ε-soft选取动作：<br><img src="http://oc9b84kcd.bkt.clouddn.com/%E8%B4%AA%E5%BF%83.png" alt=""></p>
<p><strong>On-Policy &amp; Off-Policy</strong>:<br>同策略：用于采样和改进的策略是同一个策略π。<br>异策略：用于采样的是策略μ，来改进的是策略π。<br><strong>同策略蒙特卡罗</strong><br><img src="http://oc9b84kcd.bkt.clouddn.com/OnPolicy.png" alt="image"></p>
<p><strong>异策略蒙特卡罗</strong><br><img src="http://oc9b84kcd.bkt.clouddn.com/OffPolicy.png" alt="image"></p>
<h4 id="时间差分学习"><a href="#时间差分学习" class="headerlink" title="时间差分学习"></a>时间差分学习</h4><p>蒙特卡罗需要完成一个完整的采样轨迹后再更新策略的值估计，这样没有没有充分利用强化学习任务的MDP结构。时序差分学习则结合了动态规划与蒙特卡罗方法的思想，能做到更高效的无模型学习。MC算法总是从当前状态出发，一直走到终止状态再更新当前状态的值函数，而TD算法可以从当前状态出发走n步再更新当前状态的值函数，特别的，走一步就更新的即TD(0)。TD(0)的值函数更新公式如下：<br><span>$V(S_t) = V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$</span><!-- Has MathJax --><br><span>$Q(S_t, A_t) = Q(S_t, A_t) + \alpha(R_{t+1} + \gamma Q(S_{t+1}, A&apos;) - Q(S_t, A_t)$</span><!-- Has MathJax --><br>TD(0)是指在某个状态s下执行某个动作后转移到下一个状态s′s′时，估计s′的return再更新s，假如s之后执行两次动作转移到s′′时再反回来更新s的值函数，那么就是另一种形式，从而根据step的长度n可以扩展TD到不同的形式，当step长度到达当前episode终点时就变成了MC。从而得到统一公式如下:<br><span>$G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + &hellip; + \gamma ^{n-1}R_{t+n}+\gamma ^nV(S_{t+n})$</span><!-- Has MathJax --><br><span>$V(S_t) = V(S_t) + \alpha(G_t^{(n)} - V(S_t))$</span><!-- Has MathJax --><br><strong>On-Policy Sarsa算法</strong>：<br>执行策略和改进的策略都是ε-贪婪策略。<br><img src="http://oc9b84kcd.bkt.clouddn.com/Sarsa.png" alt=""><br><strong>Off-Policy Q-Learning算法</strong>：<br>执行策略为ε-贪婪策略，改进策略为贪婪策略。<br><img src="http://oc9b84kcd.bkt.clouddn.com/QLearning.png" alt=""></p>
<h4 id="蒙特卡罗-vs-时间差分"><a href="#蒙特卡罗-vs-时间差分" class="headerlink" title="蒙特卡罗 vs 时间差分"></a>蒙特卡罗 vs 时间差分</h4><p><img src="http://oc9b84kcd.bkt.clouddn.com/MC&amp;TD.png" alt=""></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/强化学习/">强化学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/20/强化学习入门(一)/" title="强化学习入门(一)" itemprop="url">强化学习入门(一)</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-20T14:32:03.000Z" itemprop="datePublished"> 发表于 2017-12-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>书籍：机器学习 周志华<br>视频：<a href="https://www.youtube.com/watch?v=sGuiWX07sKw" target="_blank" rel="external">RL Course by David Silver</a><br>博客：<a href="http://www.algorithmdog.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B" target="_blank" rel="external">AlgorithmDog</a></p>
<h3 id="Agent-amp-Enviroment"><a href="#Agent-amp-Enviroment" class="headerlink" title="Agent &amp; Enviroment"></a>Agent &amp; Enviroment</h3><p><img src="http://oc9b84kcd.bkt.clouddn.com/Agent%20and%20Env.png" alt=""><br><strong>Agent</strong>  </p>
<ul>
<li><p>Policy（策略）：它根据当前看到的observation来决定action，是从state到action的映射。有两种表达形式，一种是Deterministic policy（确定策略）即a=π(s)a=π(s)，在某种状态s下，一定会执行某个动作a。一种是Stochastic policy（随机策略）即π(a|s)=p[At=a|St=s]π(a|s)=p[At=a|St=s]，它是在某种状态下执行某个动作的概率。   </p>
</li>
<li><p>Value function（价值函数）：它预测了当前状态下未来可能获得的reward的期望。Vπ(s)=Eπ[Rt+1+rRt+2+…|St=s]Vπ(s)=Eπ[Rt+1+rRt+2+…|St=s]。用于衡量当前状态的好坏。</p>
</li>
<li>Model（模型）：预测environment下一步会做出什么样的改变，从而预测agent接收到的状态或者reward是什么。因而有两种类型的model，一种是预测下一个state的transition  model即Pass′=p[St+1=s′|St=s,At=a]Pss′a=p[St+1=s′|St=s,At=a]，一种是预测下一次reward的reward model即Ras=E[Rt+1|St=s,At=a]Rsa=E[Rt+1|St=s,At=a]</li>
</ul>
<h3 id="马尔可夫决策-MDP"><a href="#马尔可夫决策-MDP" class="headerlink" title="马尔可夫决策 MDP"></a>马尔可夫决策 MDP</h3><blockquote>
<p>机器处于环境E中，状态空间为X,其中每个状态x是机器感知到的环境的描述，如种瓜任务上就是当前瓜苗长势的描述；机器能采取的动作构成动作空间A；机器采取的动作作用的当前状态上，会使当前环境按状态转移函数P转移到另一状态，转移过程中环境会根据潜在的奖赏函数R反馈给机器一个奖赏。综合起来,强化学习任务对应了一个四元组E=<x,a,p,r>.机器要做的是通过在环境中不断尝试而学得一个策略(policy)phi,根据这个策略，在状态x下就能得知要执行的动作a=phi(x)。</x,a,p,r></p>
</blockquote>
<p><img src="http://oc9b84kcd.bkt.clouddn.com/MDP.png" alt=""></p>
<h3 id="策略和价值"><a href="#策略和价值" class="headerlink" title="策略和价值"></a>策略和价值</h3><p><strong>策略</strong>：状态s下执行动作a的概率<br><span>$\pi(a|s) = P[A_t=a|S_t=s]$</span><!-- Has MathJax --><br><strong>奖励</strong>：当前状态下执行动作a获得的奖励的期望<br><span>$R_s^a = E[R_{t+1}|S_t=s, A_t=a]$</span><!-- Has MathJax --><br><strong>回报</strong>：回报是从t+1时刻开始的累积折扣奖励<br><span>$G_t = R_{t+1}+\gamma R_{t+2}+&hellip;=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$</span><!-- Has MathJax --><br><strong>策略</strong>：是agent的行为函数，只跟状态有关<br><span>$\pi(a|s) = \mathbb P[A_t = a | S_t = s]$</span><!-- Has MathJax --><br><strong>状态值函数</strong>:状态值函数vπ(s)是从状态s出发，按照策略π采取行为得到的期望回报<br><span>$v_\pi(s) = \mathbb E_\pi[G_t | S_t = s]$</span><!-- Has MathJax --><br><strong>行为值函数</strong>:行为值函数qπ(s,a)是从状态s出发，采取行为a后，然后按照策略π采取行为得到的期望回报<br><span>$q_\pi(s, a) = \mathbb E_\pi[G_t | S_t = s, A_t = a]$</span><!-- Has MathJax --></p>
<h3 id="Bellman期望方程"><a href="#Bellman期望方程" class="headerlink" title="Bellman期望方程"></a>Bellman期望方程</h3><p>Bellman期望方程其实就是vπ(s)和qπ(s,a)自身以及相互之间的递推关系。<br>按照状态值函数和行为值函数定义可以得到如下递推式：<br><span>$$v_\pi(s)= \mathbb E_\pi[R_{t+1}+\gamma G_{t+1} |S_t = s] \\

= \mathbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1}) | S_t = s] \\

q_\pi(s, a)= \mathbb E_\pi[G_t | S_t = s, A_t = a] \\

= \mathbb E_\pi[R_{t+1} +\gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \\$$</span><!-- Has MathJax --><br>vπ(s)与qπ(s,a)之间的关系:<br><span>$$v_\pi(s) = \sum_{a\in {A}} \pi(a|s)q_{\pi}(s, a)

q_\pi(s,a) = {R}_s^a + \gamma \sum_{s&apos; \in {S}} {P}_{ss&apos;}^a v_\pi(s&apos;)$$</span><!-- Has MathJax --><br>递推方程：<br><span>$$v_\pi(s) = \sum_{a} \pi(a|s) \left[ {R}_s^a + \gamma \sum_{s&apos;} { P}_{ss&apos;}^a v_\pi(s&apos;) \right]

q_\pi(s, a) = {R}_s^a + \gamma \sum_{s&apos;}{P}_{ss&apos;}^a \sum_{a&apos;}\pi(a&apos;|s&apos;)q_\pi(s&apos;, a&apos;)$$</span><!-- Has MathJax --><br>这两个方程的物理意义是：  </p>
<ul>
<li>当前状态下能获得的回报的期望等于当前状态下采取动作a的概率乘上采取动作a所获得的的奖励与采取状态a引起的状态转移获得的累积折扣期望回报之和。</li>
<li>当前状态下采取动作a获得的回报的期望等于采取动作a所获得的奖励与采取动作a引起的状态转移所获得的累积折扣期望回报之和。<h3 id="探索与利用-Exploration-amp-Exploitation"><a href="#探索与利用-Exploration-amp-Exploitation" class="headerlink" title="探索与利用 Exploration &amp; Exploitation"></a>探索与利用 Exploration &amp; Exploitation</h3>探索：放弃一些已知的reward信息，尝试去做一些新的选择。<br>利用： 从已知reward信息中，选择一个reward最大的。<br><strong>强化学习是在探索与利用之间达到折中。</strong>  <h4 id="ε-贪心"><a href="#ε-贪心" class="headerlink" title="ε-贪心"></a>ε-贪心</h4>ε-贪心是基于一个概率来对探索和利用进行折中：每次尝试时，以ε概率进行探索，以1-ε概率进行利用。<h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4>以一定概率进行选取，若某些动作回报高于其他，则被选取的概率也会更大。<h3 id="model-based-leanning有模型学习"><a href="#model-based-leanning有模型学习" class="headerlink" title="model-based leanning有模型学习"></a>model-based leanning有模型学习</h3>已知R~s~^a^和P~ss’~^a^时，马尔科夫决策过程就是已知的，Agent可以对环境进行建模，即有模型学习，有模型学习常用的方法有策略迭代和价值迭代。<h4 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h4>策略迭代主要步骤<strong>策略评估</strong>与<strong>策略改进</strong>，从一个随机策略出发，不断进行策略评估与策略改进，直至策略收敛。<h5 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h5>策略评估主要依据状态值函数，指的是用这个策略在当前状态下所能获得的累积奖励的期望评估该策略。<span>$v_\pi(s) = \sum_{a} \pi(a|s) \left[ {R}_s^a + \gamma \sum_{s&apos;} { P}_{ss&apos;}^a v_\pi(s&apos;) \right]$</span><!-- Has MathJax -->
<h5 id="策略改进"><a href="#策略改进" class="headerlink" title="策略改进"></a>策略改进</h5>将策略选择的动作变为当前最优的动作。<span>$\pi&apos; {\left ( x \right )}=\underset{a\in A}{argmax}\left (q^{\pi }\left ( x,a \right )  \right )$</span><!-- Has MathJax -->
迭代公式：<span>$$v_*(s) = \underset{a}{max}q_*(s,a) = \underset{a}{max} R_s^{a} + \gamma \sum_{s&apos; \in S}P_{ss&apos;}^av_*(s&apos;)

q^*(s,a) = R_s^{a} + \gamma \sum_{s&apos; \in S}P_{ss&apos;}^av_*(s&apos;) = R_s^a + \gamma \sum_{s&apos;\in S}P_{ss&apos;}^a \underset{a&apos;}    {max}q^*(s&apos;, a&apos;)$$</span><!-- Has MathJax -->
</li>
</ul>
<h4 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h4><p>策略迭代是从初始状态出发，不断评估改进直到找到到达最终状态时的回报最大的策略。价值迭代指的是从最终奖励状态开始迭代，直到找到每个状态的最大回报。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/强化学习/">强化学习</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/18/利用TensorFlow搭建CNN/" title="利用TensorFlow搭建CNN" itemprop="url">利用TensorFlow搭建CNN</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-18T13:45:03.000Z" itemprop="datePublished"> 发表于 2017-12-18</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html" target="_blank" rel="external">官方教程中文</a></p>
<h3 id="重要函数"><a href="#重要函数" class="headerlink" title="重要函数"></a>重要函数</h3><ol>
<li>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)<br>返回一个Tensor，这个输出，就是我们常说的feature map。</li>
</ol>
<ul>
<li><strong>input</strong><br>需要做卷积的输入图像，Tensor类型，具有[batch,in_height,in_width,in_channels]这样的shape，具体含义是[训练时一个batch的数量，图片的高度，图片的宽度，图像的通道数]，注意这是一个4维的Tensor，要求类型为float32和float64之一。</li>
<li><strong>filter</strong><br>相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height,filter_width,in_channels,out_channels]这样的shape,具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与input相同，其中input_channels与input相同。</li>
<li><p><strong>strides</strong><br>卷积时图像在每一维的步长，长度是4，对于图像来说，一般为[1,stride,stride,1]</p>
</li>
<li><p><strong>padding</strong><br>代表填充方式，参数只有两种，SAME和VALID，SAME比VALID的填充方式多了一列，比如一个3<em>3图像用2</em>2的滤波器进行卷积，当步长设为2的时候，会缺少一列，则进行第二次卷积的时候，VALID发现余下的窗口不足2*2会直接把第三列去掉，SAME则会填充一列，填充值为0。</p>
</li>
<li><strong>use_cudnn_on_gpu</strong><br>是否开启cudnn加速，默认为True</li>
</ul>
<ol>
<li>tf.nn.max_pool(value, ksize, strides, padding, name=None)<br>返回一个池化后的map,最大池化就是取池化窗口的最大值。</li>
</ol>
<ul>
<li>value: 池化的输入，一般池化层接在卷积层后面，所以输入通常为feature map。</li>
<li>ksize:池化窗口的大小，参数为四维向量，通常取[1,height,width,1]。</li>
<li><p><strong>strides</strong><br>卷积时图像在每一维的步长，长度是4，对于图像来说，一般为[1,stride,stride,1]</p>
</li>
<li><p><strong>padding</strong><br>代表填充方式，参数只有两种，SAME和VALID，SAME比VALID的填充方式多了一列，比如一个3<em>3图像用2</em>2的滤波器进行卷积，当步长设为2的时候，会缺少一列，则进行第二次卷积的时候，VALID发现余下的窗口不足2*2会直接把第三列去掉，SAME则会填充一列，填充值为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"><span class="comment">#获取数据集</span></div><div class="line">mnist=input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">True</span>)<span class="comment">#采用独热码</span></div><div class="line">x=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</div><div class="line">y_=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>]);</div><div class="line"><span class="comment">#定义Weight变量，输入shape,返回变量的参数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></div><div class="line">    initial=tf.truncated_normal(shape,stddev=<span class="number">0.1</span>)<span class="comment">#生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。</span></div><div class="line">    <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"><span class="comment">#定义bias变量，输入shape，返回变量的一些参数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></div><div class="line">    initial=tf.constant(<span class="number">0.1</span>,shape=shape)</div><div class="line">    <span class="keyword">return</span> tf.Variable(initial)</div><div class="line"><span class="comment">#定义卷积</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x,W)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.conv2d(x,W,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</div><div class="line"><span class="comment"># 定义池化</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.max_pool(x,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</div><div class="line"></div><div class="line"><span class="comment">#第一卷积层</span></div><div class="line">x_image=tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])<span class="comment">#把输入变成4维的，其中-1表示该维度数量让python自己算</span></div><div class="line"><span class="comment">#定义卷积核</span></div><div class="line">W_conv1=weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])<span class="comment">##卷积核大小为5*5,输入通道数为1，输出通道数为32</span></div><div class="line">b_conv1=bias_variable([<span class="number">32</span>])</div><div class="line"><span class="comment">#卷积-&gt;线性修正-&gt;池化</span></div><div class="line">h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)</div><div class="line">h_pool1=max_pool(h_conv1)</div><div class="line"></div><div class="line"><span class="comment">#第二卷积层</span></div><div class="line"><span class="comment">#卷积核</span></div><div class="line">W_conv2=weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])<span class="comment">#卷积核大小为5*5，输入通道数为第一层输出通道数32</span></div><div class="line">b_conv2=bias_variable([<span class="number">64</span>])</div><div class="line"><span class="comment">#卷积-&gt;线性修正-&gt;池化</span></div><div class="line">h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)</div><div class="line">h_pool2=max_pool(h_conv2)</div><div class="line"></div><div class="line"><span class="comment">#把特征图展平成向量，在这里是h_pool2,7*7*64，进入全连接层</span></div><div class="line">W_fc1=weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>,<span class="number">1024</span>])</div><div class="line">b_fc1=bias_variable([<span class="number">1024</span>])</div><div class="line">h_pool2_flat=tf.reshape(h_pool2,[<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</div><div class="line">h_fc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)</div><div class="line"></div><div class="line"><span class="comment">#dropout:防止过拟合，在输出层前应用随机失活</span></div><div class="line">keep_prob=tf.placeholder(tf.float32)</div><div class="line">h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)</div><div class="line"></div><div class="line"><span class="comment">#输出层</span></div><div class="line">W_fc2=weight_variable([<span class="number">1024</span>,<span class="number">10</span>])</div><div class="line">b_fc2=bias_variable([<span class="number">10</span>])</div><div class="line">y_conv=tf.matmul(h_fc1_drop,W_fc2)+b_fc2</div><div class="line"></div><div class="line"><span class="comment">#训练和评估</span></div><div class="line">cross_entropy = tf.reduce_mean(</div><div class="line">    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))</div><div class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</div><div class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">  sess.run(tf.global_variables_initializer())</div><div class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</div><div class="line">    <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">      train_accuracy = accuracy.eval(feed_dict=&#123;</div><div class="line">          x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</div><div class="line">      print(<span class="string">'step %d, training accuracy %g'</span> % (i, train_accuracy))</div><div class="line">    train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</div><div class="line"></div><div class="line">  print(<span class="string">'test accuracy %g'</span> % accuracy.eval(feed_dict=&#123;</div><div class="line">      x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</div></pre></td></tr></table></figure></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/CNN-TensorFlow/">CNN TensorFlow</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/14/MNIST机器学习入门/" title="MNIST机器学习入门" itemprop="url">MNIST机器学习入门</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-14T15:17:03.000Z" itemprop="datePublished"> 发表于 2017-12-14</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html" target="_blank" rel="external">官方教程中文</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"><span class="string">'''</span></div><div class="line">输入占位符</div><div class="line">x是一个占位符，我们在TensorFlow运行计算时输入这个值。我们希望能够输入任意数量的MNIST图像，每一张展平成784维向量。我们用2维</div><div class="line">浮点数张量来表示这些图，这个张量的形状是[None,784],None表示此张量的第一个维度可以是任何长度的。</div><div class="line">'''</div><div class="line">x=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</div><div class="line">y_real=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</div><div class="line"><span class="comment">#权重及偏差</span></div><div class="line">W=tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</div><div class="line">b=tf.Variable(tf.zeros([<span class="number">10</span>]))</div><div class="line">y=tf.nn.softmax(tf.matmul(x,W)+b)</div><div class="line"></div><div class="line"><span class="comment">#交叉熵</span></div><div class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_real* tf.log(y), reduction_indices=[<span class="number">1</span>]))</div><div class="line"></div><div class="line"><span class="comment">#优化目标</span></div><div class="line">train_step=tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</div><div class="line"></div><div class="line"><span class="comment">#初始化</span></div><div class="line">init=tf.global_variables_initializer()</div><div class="line">sess=tf.Session()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"><span class="comment">#训练</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    batch_xs,batch_ys=mnist.train.next_batch(<span class="number">100</span>)</div><div class="line">    sess.run(train_step,feed_dict=&#123;x:batch_xs,y_real:batch_ys&#125;)</div><div class="line">    <span class="keyword">if</span> i%<span class="number">50</span>==<span class="number">0</span>:</div><div class="line">        correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_real,<span class="number">1</span>))<span class="comment">#tf.argmax(tensor,axis)返回tensor在某个维度上最大值的下标，1表示按行</span></div><div class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">        print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_real: mnist.test.labels&#125;))</div></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/MNIST-TensorFlow/">MNIST TensorFlow</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/10/利用TensorFlow搭建神经网络/" title="利用TensorFlow搭建神经网络" itemprop="url">利用TensorFlow搭建神经网络</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-10T15:17:03.000Z" itemprop="datePublished"> 发表于 2017-12-10</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>相关理论可以看周志华的《机器学习》,重点在于BP算法的推导。代码参照<br><a href="https://www.youtube.com/watch?v=RSRkp8VAavQ" target="_blank" rel="external">视频</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment">#添加层函数,返回该层输出</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,in_size,out_size,activation_function=None)</span>:</span></div><div class="line">    Weights=tf.Variable(tf.random_normal([in_size,out_size]))<span class="comment">#权重变量</span></div><div class="line">    biases=tf.Variable(tf.zeros([<span class="number">1</span>,out_size]))<span class="comment">#阈值变量</span></div><div class="line">    Wx_plus_b=tf.matmul(inputs,Weights)+biases</div><div class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        outputs=Wx_plus_b</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        outputs=activation_function(Wx_plus_b)</div><div class="line">    <span class="keyword">return</span> outputs</div><div class="line"></div><div class="line"><span class="comment">#创建一些数据</span></div><div class="line">x_data=np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:,np.newaxis]<span class="comment">#x_data是[-1,1]之间的等差数列</span></div><div class="line">noise=np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>,x_data.shape)<span class="comment">#均值为0，标准差为0.05的噪声</span></div><div class="line">y_data=np.square(x_data)<span class="number">-0.5</span>+noise<span class="comment">#y=x^2-0.5+n</span></div><div class="line"></div><div class="line"><span class="comment"># 定义占位符</span></div><div class="line">xs=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</div><div class="line">ys=tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</div><div class="line"><span class="comment">#添加隐藏层</span></div><div class="line">ll=add_layer(xs,<span class="number">1</span>,<span class="number">10</span>,activation_function=tf.nn.relu)</div><div class="line"><span class="comment">#添加输出层</span></div><div class="line">prediction=add_layer(ll,<span class="number">10</span>,<span class="number">1</span>,activation_function=<span class="keyword">None</span>)</div><div class="line"></div><div class="line"><span class="comment">#定义优化目标和优化器</span></div><div class="line">loss=tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[<span class="number">1</span>]))<span class="comment">#降低累积均方误差</span></div><div class="line">train_step=tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</div><div class="line"></div><div class="line"><span class="comment">#初始化所有变量</span></div><div class="line">init=tf.initialize_all_variables()</div><div class="line">sess=tf.Session()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line"><span class="comment">#绘制真实曲线</span></div><div class="line">fig=plt.figure()</div><div class="line">ax=fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div><div class="line">ax.scatter(x_data,y_data)</div><div class="line">plt.ion()</div><div class="line">plt.show()</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">    <span class="comment">#训练</span></div><div class="line">    sess.run(train_step,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</div><div class="line">    <span class="keyword">if</span> i%<span class="number">50</span>==<span class="number">0</span>:</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            ax.lines.remove(lines[<span class="number">0</span>])</div><div class="line">        <span class="keyword">except</span> Exception:</div><div class="line">            <span class="keyword">pass</span></div><div class="line">        prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)</div><div class="line">        <span class="comment"># plot the prediction</span></div><div class="line">        lines = ax.plot(x_data, prediction_value, <span class="string">'r-'</span>, lw=<span class="number">5</span>)</div><div class="line">        plt.pause(<span class="number">0.1</span>)</div></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/神经网络-TensorFlow/">神经网络 TensorFlow</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/05/TensorFlow快速入门/" title="TensorFlow快速入门" itemprop="url">TensorFlow快速入门</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-12-05T12:17:03.000Z" itemprop="datePublished"> 发表于 2017-12-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><ol>
<li><p>Tensor张量：类似于Numpy中的ndarray<br>rank:维度数量<br>shape:每个维度的大小</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">import tensorflow <span class="keyword">as</span> <span class="keyword">tf</span></div><div class="line">&gt;&gt;&gt; <span class="keyword">a</span>=<span class="keyword">tf</span>.constant(<span class="number">5</span>)</div><div class="line">&gt;&gt;&gt; <span class="keyword">b</span>=<span class="keyword">tf</span>.constant(<span class="number">3</span>)</div><div class="line">&gt;&gt;&gt; <span class="keyword">c</span>=<span class="keyword">tf</span>.multiply(<span class="keyword">a</span>,<span class="keyword">b</span>)</div><div class="line">&gt;&gt;&gt; d=<span class="keyword">tf</span>.<span class="built_in">add</span>(<span class="keyword">a</span>,<span class="keyword">b</span>)</div><div class="line">&gt;&gt;&gt; <span class="keyword">e</span>=<span class="keyword">tf</span>.<span class="built_in">add</span>(<span class="keyword">c</span>,d)</div><div class="line">&gt;&gt;&gt; sess=<span class="keyword">tf</span>.Session()</div><div class="line">&gt;&gt;&gt; <span class="keyword">print</span>(sess.run(<span class="keyword">e</span>))</div><div class="line"><span class="number">23</span></div><div class="line">&gt;&gt;&gt; sess.<span class="keyword">close</span>()</div><div class="line"></div><div class="line">#常用张量</div><div class="line"><span class="keyword">a</span>=<span class="keyword">tf</span>.zeros([<span class="number">2</span>,<span class="number">3</span>],<span class="keyword">tf</span>.int32)</div><div class="line"><span class="keyword">b</span>=<span class="keyword">tf</span>.ones([<span class="number">2</span>,<span class="number">3</span>],<span class="keyword">tf</span>.int32)</div><div class="line"></div><div class="line">#随机张量</div><div class="line">&gt;&gt;&gt; <span class="keyword">a</span>=<span class="keyword">tf</span>.random_normal([<span class="number">2</span>,<span class="number">2</span>],mean=<span class="number">0.0</span>,stddev=<span class="number">1.0</span>)</div><div class="line">&gt;&gt;&gt; <span class="keyword">b</span>=<span class="keyword">tf</span>.random_uniform([<span class="number">2</span>,<span class="number">2</span>],minval=<span class="number">0.0</span>,maxval=<span class="number">1.0</span>)</div><div class="line">&gt;&gt;&gt; <span class="keyword">c</span>=<span class="keyword">a</span>+<span class="keyword">b</span></div><div class="line">&gt;&gt;&gt; with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></div><div class="line">...     <span class="keyword">print</span>(sess.run(<span class="keyword">c</span>))</div><div class="line"></div><div class="line">#占位符：具体计算时再传入数据</div><div class="line"><span class="keyword">a</span>=<span class="keyword">tf</span>.placeholder(<span class="keyword">tf</span>.int32,[<span class="number">3</span>,])#定义一个向量占位符</div><div class="line">with <span class="keyword">tf</span>.Session() <span class="keyword">as</span> ses<span class="variable">s:</span></div><div class="line"><span class="keyword">print</span>(sess.run(<span class="keyword">a</span>,feed_dict=&#123;<span class="variable">a:</span>[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]&#125;))</div></pre></td></tr></table></figure>
</li>
<li><p>变量tf.Variable()</p>
</li>
</ol>
<ul>
<li><p>创建：用一个张量作为初始值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">a=tf.Variable([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">12</span>]])<span class="comment">#初始值为[[2,3],[1,2]]</span></div></pre></td></tr></table></figure>
</li>
<li><p>初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">init=tf.global_variables_initializer()<span class="comment">#初始化算子</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">sess.run(init)<span class="comment">#run的参数是一个算子</span></div></pre></td></tr></table></figure>
</li>
<li><p>赋值算子 assign()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>a=tf.Variable([<span class="number">1</span>,<span class="number">1</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>b=tf.Variable([<span class="number">2</span>,<span class="number">2</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>assign_op=a.assign(b)<span class="comment">#赋值算子</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>init=tf.global_variables_initializer()<span class="comment">#初始化算子</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line"><span class="meta">... </span>    sess.run(init)</div><div class="line"><span class="meta">... </span>    print(sess.run(a))</div><div class="line"><span class="meta">... </span>    sess.run(assign_op)</div><div class="line"><span class="meta">... </span>    print(sess.run(a))</div><div class="line"><span class="meta">... </span></div><div class="line">[<span class="number">1</span> <span class="number">1</span>]</div><div class="line">array([<span class="number">2</span>, <span class="number">2</span>])</div><div class="line">[<span class="number">2</span> <span class="number">2</span>]</div><div class="line">&gt;&gt;&gt;</div></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li><p>自动梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x=tf.constant([<span class="number">2.0</span>,<span class="number">1.0</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y=tf.constant([<span class="number">1.0</span>,<span class="number">2.0</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z=x*y+x*x</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>dx,dy=tf.gradients(z,[x,y])<span class="comment">#求Z关于x,y的偏导数</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line"><span class="meta">... </span>    dx_v,dy_v=sess.run([dx,dy])</div><div class="line"><span class="meta">... </span>    print(dx_v)</div><div class="line"><span class="meta">... </span>    print(dy_v)</div></pre></td></tr></table></figure>
</li>
<li><p>优化器<br>大多数机器学习的任务就是<strong>最小化损失</strong>，在损失定义的情况下，后面的工作就交给优化器了。因为深度学习常见的是基于梯度的优化，也就是说，优化器最后其实是各种对于梯度下降算法的优化。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">class tf.train.GradientDescentOptimizer</div><div class="line">这个类是实现梯度下降算法的优化器，构造函数主要设置一下学习率。</div><div class="line">__init__(learning_rate, use_locking=False,name=’GradientDescent’)</div><div class="line"></div><div class="line">作用：创建一个梯度下降优化器对象 </div><div class="line">参数： </div><div class="line">learning_rate: A Tensor or a floating point value. 要使用的学习率 </div><div class="line">use_locking: 要是True的话，就对于更新操作（<span class="keyword">update</span> operations.）使用锁 </div><div class="line"><span class="keyword">name</span>: 名字，可选，默认是”GradientDescent”.</div><div class="line">compute_gradients(loss,var_list=<span class="keyword">None</span>,gate_gradients=GATE_OP,aggregation_method=<span class="keyword">None</span>,colocate_gradients_with_ops=<span class="literal">False</span>,grad_loss=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">作用：对于在变量列表（var_list）中的变量计算对于损失函数的梯度,这个函数返回一个（梯度，变量）对的列表，其中梯度就是相对应变量的梯度了。这是minimize()函数的第一个部分， </div><div class="line">参数： </div><div class="line">loss: 待减小的值 </div><div class="line">var_list: 默认是在GraphKey.TRAINABLE_VARIABLES. </div><div class="line">gate_gradients: How <span class="keyword">to</span> gate the computation <span class="keyword">of</span> gradients. Can be GATE_NONE, GATE_OP, <span class="keyword">or</span> GATE_GRAPH. </div><div class="line">aggregation_method: Specifies the method used <span class="keyword">to</span> combine gradient terms. Valid <span class="keyword">values</span> <span class="keyword">are</span> defined <span class="keyword">in</span> the <span class="keyword">class</span> AggregationMethod. </div><div class="line">colocate_gradients_with_ops: <span class="keyword">If</span> <span class="literal">True</span>, try colocating gradients <span class="keyword">with</span> the <span class="keyword">corresponding</span> op. </div><div class="line">grad_loss: Optional. A Tensor holding the gradient computed <span class="keyword">for</span> loss.</div><div class="line">apply_gradients(grads_and_vars,global_step=<span class="keyword">None</span>,<span class="keyword">name</span>=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">作用：把梯度“应用”（<span class="keyword">Apply</span>）到变量上面去。其实就是按照梯度下降的方式加到上面去。这是minimize（）函数的第二个步骤。 返回一个应用的操作。 </div><div class="line">参数: </div><div class="line">grads_and_vars: compute_gradients()函数返回的(gradient, <span class="keyword">variable</span>)对的列表 </div><div class="line">global_step: Optional <span class="keyword">Variable</span> <span class="keyword">to</span> <span class="keyword">increment</span> <span class="keyword">by</span> one <span class="keyword">after</span> the <span class="keyword">variables</span> have been updated. </div><div class="line"><span class="keyword">name</span>: 可选，名字</div><div class="line">get_name()</div><div class="line"></div><div class="line">minimize(loss,global_step=<span class="keyword">None</span>,var_list=<span class="keyword">None</span>,gate_gradients=GATE_OP,aggregation_method=<span class="keyword">None</span>,colocate_gradients_with_ops=<span class="literal">False</span>,<span class="keyword">name</span>=<span class="keyword">None</span>,grad_loss=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">作用：非常常用的一个函数 </div><div class="line">通过更新var_list来减小loss，这个函数就是前面compute_gradients() 和apply_gradients().的结合</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>TensorFlow训练线性回归模型。<br><figure class="highlight autoit"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"><span class="meta">#创建100个数据x,y,y=x*0.1+0.3</span></div><div class="line">x_data=np.<span class="built_in">random</span>.rand(<span class="number">100</span>).astype(np.float32)</div><div class="line">y_data=x_data*<span class="number">0.1</span>+<span class="number">0.3</span></div><div class="line"></div><div class="line"><span class="meta">#尝试估计W和b使它们满足y_data=W*x_data+b</span></div><div class="line">W=tf.Variable(tf.random_uniform([<span class="number">1</span>],<span class="number">-1.0</span>,<span class="number">1.0</span>))</div><div class="line">b=tf.Variable(tf.zeros([<span class="number">1</span>]))</div><div class="line">y=W*x_data+b</div><div class="line"></div><div class="line"><span class="meta">#采用最小均方误差估计</span></div><div class="line">loss=tf.reduce_mean(tf.square(y-y_data))<span class="meta">#loss为优化目标函数，有点像lingo里的</span></div><div class="line">optimizer=tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)<span class="meta">#创建一个梯度下降优化器，学习率设置为0.5</span></div><div class="line">train=optimizer.minimize(loss)<span class="meta">#最小化均方误差</span></div><div class="line"></div><div class="line"><span class="meta">#打开会话，初始化变量</span></div><div class="line">init=tf.global_variables_initializer()</div><div class="line">sess=tf.Session()</div><div class="line">sess.<span class="built_in">run</span>(init)</div><div class="line"></div><div class="line"><span class="meta">#开始训练</span></div><div class="line"><span class="keyword">for</span> <span class="keyword">step</span> <span class="keyword">in</span> range(<span class="number">201</span>):</div><div class="line">    sess.<span class="built_in">run</span>(train)</div><div class="line">    <span class="keyword">if</span>(<span class="keyword">step</span>%<span class="number">20</span>==<span class="number">0</span>):</div><div class="line">        print(<span class="keyword">step</span>,sess.<span class="built_in">run</span>(W),sess.<span class="built_in">run</span>(b))</div></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/深度学习-TensorFlow/">深度学习 TensorFlow</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/11/19/母函数法/" title="母函数法" itemprop="url">母函数法</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-11-19T12:17:03.000Z" itemprop="datePublished"> 发表于 2017-11-19</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="母函数法"><a href="#母函数法" class="headerlink" title="母函数法"></a>母函数法</h2><blockquote>
<p>母函数即生成函数，构造这么一个多项式函数g(x)，使得x的n次方系数为f(n)，是组合数学中尤其是计数方面的一个重要理论和工具。<br><strong>母函数把组合问题的加法法则和幂级数的乘幂对应起来。</strong></p>
</blockquote>
<p>首先看一个多项式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(1+a_1x)(1+a_2x)(1+a_3x)...(1+a_nx)=1+(a_1+a_2+...+a_n)x+(a_1a_2+a_1a_3+...+a_&#123;n-1&#125;a_n)x^2+...+a_1a_2a_3...a_nx^n</div></pre></td></tr></table></figure></p>
<p>其中x^1 的系数就是从a1,…,an中选1个的组合数，x^2的系数就是从a1,…,an中选两个的组合数，…。  </p>
<h3 id="1-砝码称重"><a href="#1-砝码称重" class="headerlink" title="1.砝码称重"></a>1.砝码称重</h3><blockquote>
<p>有1克、2克、3克、4克的砝码各一枚，能称出哪几种重量？每种重量各有几种可能方案？</p>
</blockquote>
<p>我们可以构造母函数来求解,假设x代表砝码，x的指数代表其重量：<br>1个1克砝码：1<em> x^0+1</em> x^1<br>1个2克砝码：1<em> x^0+1</em> x^2<br>1个3克法码：1<em> x^0+1</em> x^3<br>1个4克砝码：1<em> x^0+1</em> x^4  </p>
<p>注意，每个砝码有和不选两种状态，不选的时候指数为0，所以有1*x^0一项。</p>
<p>原问题是一个组合数学加法问题，按照我们的定义，现在x的指数表示砝码重量，所以对x的多项式相乘便得到原来加法的组合：(1+x)(1+x^ 2)(1+x^ 3)(1+x^ 4)=1+x+x^2+2x^ 3+2x^ 4+2x^ 5+2x^ 6+2x^ 7+x^ 8+x^ 9+x^10<br>展开后的多项式中x的指数就是可以秤出的重量，系数就是方案数。<br>例如：2x^4表示4克有两种方案：1+3或者4</p>
<blockquote>
<p>假设上面每种砝码都有无穷多个，那么称出4克有多少种方案？</p>
</blockquote>
<p>这个问题和原问题的区别在于每种砝码可选的状态更多了，选0个，1个，2个….<br>1克砝码：1+x+x^2^+x^3^+x^4^+….<br>2克砝码：1+x^2^+x^4^+…..<br>3克砝码：1+x^3^+….<br>4克砝码：1+x^4^+….  </p>
<p>既然题目问4克的方案，我们只需要把每种砝码到指数为4的多项式写出来如下：</p>
<p>(1+x+x^2^+x^3^+x^4^)(1+x^2^+x^4^)(1+x^3^)(1+x^4^)  </p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><p>(1)输入：不同砝码种类num,砝码重量数组w[]<br>输出：可以称出的重量即其方案数</p>
<p>(2)数据结构：<br>定义一个数组c[]保存最终多项式,其中c(i)为指数为i的项的系数，例如例1中c[]={1,1,1,2,2,2,2,2,1,1,1},即1+x+x^2^+2x^3^+2x^4^+2x^5^+2x^6^+2x^7^+x^8^+x^9^+x^10^</p>
<p>(3)算法描述：</p>
<p><1>.建立数组c[maxSize],并将c[0]初始化为1，c[w[0]]初始化为1；</1></p>
<p><2><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;num;++i)<span class="comment">//依次遍历w[]中每一项，模拟矩阵相乘</span></div><div class="line"> <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;maxSize;++j)</div><div class="line"> &#123;</div><div class="line">	<span class="keyword">if</span>(c[j]!=<span class="number">0</span>)<span class="comment">//和系数不为0的相乘</span></div><div class="line">	&#123;</div><div class="line">		<span class="keyword">int</span> temp=w[i]+j;<span class="comment">//两项相乘，得到一个新指数</span></div><div class="line">		<span class="keyword">if</span>(temp&lt;maxSize)</div><div class="line">		&#123;</div><div class="line">			c[temp]++;<span class="comment">//系数加1</span></div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure></2></p>
<p> (4)源码：<br> <figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/*生成函数应用*/</span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> MaxSize 11</span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">int</span> c[MaxSize];<span class="comment">//存储最终多项式</span></div><div class="line"><span class="keyword">int</span> cache[MaxSize];<span class="comment">//缓存中间计算结果</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">Coin</span><span class="params">(<span class="keyword">int</span> w[], <span class="keyword">int</span> num)</span></span></div><div class="line">&#123;</div><div class="line">	c[<span class="number">0</span>] = <span class="number">1</span>;</div><div class="line">	c[w[<span class="number">0</span>]] = <span class="number">1</span>;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; num;++i)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; MaxSize;++j)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">if</span> (c[j] != <span class="number">0</span>)</div><div class="line">			&#123;</div><div class="line">				<span class="keyword">int</span> temp = w[i] + j;</div><div class="line">				<span class="keyword">if</span> (temp &lt; MaxSize)</div><div class="line">				&#123;</div><div class="line">					cache[temp]++;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; MaxSize; ++k)</div><div class="line">		&#123;</div><div class="line">			c[k] += cache[k];</div><div class="line">			cache[k] = <span class="number">0</span>;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; MaxSize; ++i)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">if</span> (c[i] != <span class="number">0</span>)</div><div class="line">		&#123;</div><div class="line">			<span class="built_in">cout</span> &lt;&lt; <span class="string">"组成质量为"</span> &lt;&lt; i &lt;&lt; <span class="string">"克的方案数有"</span> &lt;&lt; c[i] &lt;&lt; <span class="string">"种"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/母函数法/">母函数法</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/11/03/容斥原理/" title="容斥原理" itemprop="url">容斥原理</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-11-03T15:00:00.000Z" itemprop="datePublished"> 发表于 2017-11-03</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="容斥原理：加奇减偶"><a href="#容斥原理：加奇减偶" class="headerlink" title="容斥原理：加奇减偶"></a>容斥原理：加奇减偶</h3><p>要计算几个集合并集的大小，我们要先将单个集合的大小计算出来，然后减去两个集合相交的部分，再加回所有三个集合相交的部分，再减去所有四个集合相交的部分，依此类推，一直计算到所有集合相交的部分。</p>
<h3 id="例1：互质数"><a href="#例1：互质数" class="headerlink" title="例1：互质数"></a>例1：<a href="http://acm.hdu.edu.cn/showproblem.php?pid=4135" target="_blank" rel="external">互质数</a></h3><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>题目要求的是a~b之间有多少是与N互质的。暴力算法可以遍历区间[a,b]看和N的最大公约数是不是1，但这样肯定会超时。我们可以先预处理[1,M]中和N互质的，要求互质，可以先求出不互质的，再用M减去即可。对于和N不互质的，必定和N有公因子，先将N分解，N=n0^m0^n1^m1^…,则n0的倍数，n1的倍数…均和N不互质，但是我们发现对于不同因子的倍数，有可能是相同的，比如因子2和3，会出现2的三倍等于3的两倍，于是这里还的应用容斥原理，加奇减偶，去掉重复加上的。最后，问题转化成了求(b-func(b))  -  (a-1-func(a-1))，func(b)代表1~b中与N不互素的个数，自然b-func(b)就代表的是1~b中与N互素的个数。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> ll;</div><div class="line"></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">70</span>;</div><div class="line"></div><div class="line">ll A, B, N;</div><div class="line"></div><div class="line">ll divisor[maxn];<span class="comment">//不同因子</span></div><div class="line"></div><div class="line"><span class="comment">/*计算1~m中包含这n个因子的数的个数*/</span></div><div class="line"><span class="function">ll <span class="title">calc</span><span class="params">(ll m, ll n)</span></span></div><div class="line">&#123;</div><div class="line">	ll ans = <span class="number">0</span>, tmp, i, j, flag;</div><div class="line">	<span class="keyword">for</span> (i = <span class="number">1</span>; i &lt; (ll)<span class="number">1</span> &lt;&lt; n; ++i)<span class="comment">//通过位运算枚举集合中的组合，这个用法要记住</span></div><div class="line">	&#123;</div><div class="line">		tmp = <span class="number">1</span>, flag = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; n; ++j)</div><div class="line">		&#123;</div><div class="line">			<span class="keyword">if</span> (i&amp;(ll)<span class="number">1</span> &lt;&lt; j)<span class="comment">//判断第j个因子是否被用到</span></div><div class="line">				++flag, tmp *= divisor[j];</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (flag &amp; <span class="number">1</span>)ans += m / tmp;<span class="comment">//容斥原理，奇加偶减</span></div><div class="line">		<span class="keyword">else</span> ans -= m / tmp;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> ans;</div><div class="line">&#125;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">divide</span><span class="params">(ll x)</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> n = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> (ll i = <span class="number">2</span>; i*i &lt;= x; ++i)</div><div class="line">	&#123;</div><div class="line">		<span class="keyword">if</span> (x&amp;&amp;x%i == <span class="number">0</span>)</div><div class="line">		&#123;</div><div class="line">			divisor[n++] = i;</div><div class="line">			<span class="keyword">while</span> (x&amp;&amp;x%i == <span class="number">0</span>)x /= i;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">if</span> (x &gt; <span class="number">1</span>)divisor[n++] = x;</div><div class="line">	<span class="keyword">return</span> n;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">	<span class="keyword">int</span> t; <span class="built_in">scanf</span>(<span class="string">"%d\n"</span>,&amp;t);</div><div class="line">	<span class="keyword">while</span> (t--)</div><div class="line">	&#123;</div><div class="line">		<span class="built_in">scanf</span>(<span class="string">"%lld%lld%lld"</span>,&amp;A,&amp;B,&amp;N);</div><div class="line">		<span class="keyword">int</span> n = divide(N);</div><div class="line">		ll res = B - calc(B, n) - (A - <span class="number">1</span> - calc(A - <span class="number">1</span>, n));</div><div class="line">		<span class="built_in">printf</span>(<span class="string">"%lld\n"</span>,res);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/容斥原理/">容斥原理</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/10/22/拓扑排序/" title="拓扑排序" itemprop="url">拓扑排序</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="洛绍川" target="_blank" itemprop="author">洛绍川</a>
		
  <p class="article-time">
    <time datetime="2017-10-22T11:33:00.000Z" itemprop="datePublished"> 发表于 2017-10-22</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>将有向图中的顶点以线性方式进行排序。即对于任何连接自顶点u到顶点v的有向边uv,在最后的排序结果中，顶点u总是在顶点v的前面。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ol>
<li>从DAG图中选择一个没有前驱(即入度为0)的顶点并输出。</li>
<li>从图中删除该顶点和所有以它为起点的有向边。</li>
<li>重复1和2直至当前DAG图为空或当前图中不存在无前驱的顶点为止。后一种情况说明有向图中必然存在环。</li>
</ol>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p><a href="https://leetcode.com/problems/course-schedule/description/" target="_blank" rel="external">207. Course Schedule</a><br><strong>BFS</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; din(numCourses,<span class="number">0</span>);</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; G(numCourses);</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> e:prerequisites)</div><div class="line">        &#123;</div><div class="line">            ++din[e.second];</div><div class="line">            G[e.first].push_back(e.second);</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; que;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;numCourses;++i)&#123;</div><div class="line">            <span class="keyword">if</span>(!din[i])que.push(i);</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">while</span>(!que.empty())&#123;</div><div class="line">            <span class="keyword">int</span> t=que.front();</div><div class="line">            que.pop();</div><div class="line">            <span class="keyword">for</span>(<span class="keyword">auto</span> s:G[t])&#123;</div><div class="line">                --din[s];</div><div class="line">                <span class="keyword">if</span>(!din[s])que.push(s);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> d:din)</div><div class="line">            <span class="keyword">if</span>(d)<span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure></p>
<p><strong>DFS</strong><br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">L ← Empty list that will contain <span class="keyword">the</span> sorted nodes</div><div class="line"><span class="keyword">while</span> there are unmarked nodes <span class="built_in">do</span></div><div class="line">    select <span class="keyword">an</span> unmarked node n</div><div class="line">    visit(n) </div><div class="line"></div><div class="line"> <span class="function"><span class="keyword">function</span> <span class="title">visit</span>(<span class="title">node</span> <span class="title">n</span>)</span></div><div class="line">    <span class="keyword">if</span> n has <span class="keyword">a</span> permanent mark <span class="keyword">then</span> <span class="literal">return</span></div><div class="line">    <span class="keyword">if</span> n has <span class="keyword">a</span> temporary mark <span class="keyword">then</span> <span class="built_in">stop</span> (<span class="keyword">not</span> <span class="keyword">a</span> DAG)</div><div class="line">    mark n temporarily</div><div class="line">    <span class="keyword">for</span> <span class="keyword">each</span> node m <span class="keyword">with</span> <span class="keyword">an</span> edge <span class="built_in">from</span> n <span class="built_in">to</span> m <span class="built_in">do</span></div><div class="line">        visit(m)</div><div class="line">    mark n permanently</div><div class="line">    <span class="built_in">add</span> n <span class="built_in">to</span> head <span class="keyword">of</span> L</div></pre></td></tr></table></figure></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> Solution &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;G,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;visit,<span class="keyword">int</span> u)</span></span>&#123;</div><div class="line">        <span class="keyword">if</span>(visit[u]==<span class="number">1</span>)<span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">        <span class="keyword">if</span>(visit[u]==<span class="number">-1</span>)<span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        visit[u]=<span class="number">-1</span>;</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> v:G[u])</div><div class="line">            <span class="keyword">if</span>(!dfs(G,visit,v))<span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        visit[u]=<span class="number">1</span>;</div><div class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; visit(numCourses,<span class="number">0</span>);</div><div class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; G(numCourses);</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> e:prerequisites)G[e.first].push_back(e.second);</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> u=<span class="number">0</span>;u&lt;numCourses;++u)</div><div class="line">            <span class="keyword">if</span>(!dfs(G,visit,u))<span class="keyword">return</span> <span class="literal">false</span>;</div><div class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</div><div class="line">    &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/图论-拓扑排序/">图论 拓扑排序</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  


  
<div class="categorieslist">
	<p class="asidetitle">分类</p>
		<ul>
		
		  
			<li><a href="/categories/PAT/" title="PAT">PAT<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Parallel-Studio/" title="Parallel Studio">Parallel Studio<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Python/" title="Python">Python<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/hihocoder/" title="hihocoder">hihocoder<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/生活随笔/" title="生活随笔">生活随笔<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法/" title="算法">算法<sup>3</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/算法-leetcode/" title="算法 leetcode">算法 leetcode<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/算法-动态规划/" title="算法 动态规划">算法 动态规划<sup>8</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Python/" title="Python">Python<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Kickstart-Google/" title="Kickstart Google">Kickstart Google<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/笔试/" title="笔试">笔试<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/强化学习/" title="强化学习">强化学习<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/KMP算法/" title="KMP算法">KMP算法<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/DFA/" title="DFA">DFA<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/PAT/" title="PAT">PAT<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Manacher算法/" title="Manacher算法">Manacher算法<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/正则表达式/" title="正则表达式">正则表达式<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Parallel-Studio/" title="Parallel Studio">Parallel Studio<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/ICC/" title="ICC">ICC<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/MKL/" title="MKL">MKL<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Trie树/" title="Trie树">Trie树<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hihocoder-分治/" title="hihocoder 分治">hihocoder 分治<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/深度学习-TensorFlow/" title="深度学习 TensorFlow">深度学习 TensorFlow<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/C/" title="C">C<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/线段树/" title="线段树">线段树<sup>1</sup></a></li>
			
		
		</ul>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2018 
		
		<a href="/about" target="_blank" title="洛绍川">洛绍川</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End --><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

  </body>
 </html>
